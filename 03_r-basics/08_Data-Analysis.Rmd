---
title: 'Data Analysis in R: Lecture Notes'
author: "PS239T"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
    df_print: paged
    theme: flatly
    highlight: pygments
---

For most applied research, conducting data analysis in R involves the following core tasks: 

1. [Constructing](#constructing-a-dataset) a dataset
2. [Summarizing](#summarizing) the structure and content of data
3. Carrying out operations and calculations across [groups](#calculating-across-groups)
4. [Reshaping](#reshaping) data to and from various formats
5. Testing relationships between variables, either [descriptive](#description) or [causal](#inferences) 


# 0) SET UP 

```{r}
# remove all objects
rm(list=ls())
# note that this does not remove any packages you might already have called, just objects

# # Install pacman if necessary (uncomment to do so)
# install.packages("pacman")  

# # Alternate installation 
# install.packages("pacman", "devtools")  
# library(devtools)
# install_github("trinker/pacman")

# Unload all packages 
library(pacman)
pacman::p_unload(all)

# Add packages 
pacman::p_load(
  tidyverse, #dplyr, readr, etc.
  data.table, #fread() 
  foreign, #load data types including stata .dta files 
  magrittr, #%<>% operator
  skimr #for summerising
)
```

** Tips **

But also be aware that this using `rm` and `setwd` are not best practices because they can create difficulties for replication. For instance, `rm` is an illusion. It only cleans up objects, so it's different from a fresh session. Other uses might find difficult to replicate your code if your code have dependencies. Similarly, `setwd` does not work because it makes your project *un*portable. For more advanced projects, I recommend you to learn using [here package](https://cran.r-project.org/web/packages/here/index.html). For more information on self-contained projects, see [this post](https://www.tidyverse.org/articles/2017/12/workflow-vs-script/).


# 1) LOAD & CLEAN DATASET {#constructing-a-dataset}

The first thing we want to do is load a dataset. This usually involves one or more of the following tasks:

a) ***Importing*** different types of data from different sources
b) ***Cleaning*** the data, including subsetting, altering values, etc.
c) ***Merging*** the data with other data

## 1.1. Importing Data

R can load almost any kind of data, but depending on your data type, you may need to load additional packages. For small and medium datasets, check the following website to see what functions and packages to use: https://www.statmethods.net/input/exportingdata.html.

For large datasets, speed is a concern. Google the fastest way to import your data type and look for recent stackoverflow or blog posts to find the best package. 

** Tips **

- For files smaller than 1MB use read.csv().
- For files larger than 100MB use fread() from data.table package and read_csv() from readr package 
- Also, use rmd (native R file extension) rather than csv for large files.

### 1.1.1. CSV File 

*Base R*: Import country-year data with header row, values separated by ",", decimals as ".". Note that characters are automatically converted to factors. 
```{r}
dat_baser <- read.csv(file = "data/country-year.csv") # stringsAsFactors = F
glimpse(dat_baser)
```

*Tidyverse*: faster than read.csv and does not automatically convert factors 
```{r}
# Tidyverse
dat_readr <- readr::read_csv("data/country-year.csv")
glimpse(dat_readr)
```

*data.table()*: fread is the fastest way to import large csv datasets in R and does not automatically convert factors 
```{r}
# data.table() 
dat_dt <- fread("data/country-year.csv")
glimpse(dat_dt)
```

Now, let's clean up our environment and save the .csv as different formats, so we can practice loading them. 
```{r, eval=F}
# Stata file 
foreign::write.dta(dat_baser, "data/country-year.dta")

# RData file 
save(dat_baser, file="data/country-year.RData")
save(dat_baser, dat_readr, file="data/country-year.RData")

# RDS file 
saveRDS(dat_baser, file="data/country-year.rds")

# Clean up environment 
rm(dat_baser, dat_readr, dat_dt)
```

### 1.1.2. Stata file 

```{r}
# Load stata .dta file 
dat_dta <- foreign::read.dta("data/country-year.dta")

# Note that periods (.) are coverted to underscores (_)
glimpse(dat_dta)
```

### 1.1.3. RData  and RDS

```{r}
# RData
load("data/country-year.RData")

# Note you cannot save .RData objects under new name when loading
dat <- load("data/country-year.RData")

# RDS
dat <- readRDS("data/country-year.rds")
```


### 1.1.4. Import from web address

```{r}
fema_dat <- read_csv("https://www.fema.gov/api/open/v1/FemaWebDisasterDeclarations.csv")
glimpse(fema_dat)
```

### 1.1.5. Exercise 

First, clean up the environment 
```{r}
# remove all objects
rm(list=ls())
```

Let's load the PolityVI (autocracy and democracy scales) and CIRI Human Rights Project datasets (both csvs):

* Open "data/Polity/p4v2013.csv" and save as object called `polity` 
* Open "data/CIRI/CIRI_1981_2011.csv" and save as object called ciri

```{r, eval=F}
# Import polity dataset 
polity <- ______________________

# Look at 6 rows using head() or sample_n()
head(_______)
sample_n(________, __)

# Look at data with glimpse()
glimpse(_______)

# Open other dataset 
ciri <- ____________________

# Look at 6 rows using head() or sample_n()
head(_______)
sample_n(________, __)

# Look at data with glimpse()
glimpse(_______)
```


Solutions: 
```{r}
# Import polity dataset 
polity <- read_csv( "data/Polity/p4v2013.csv")

# Import 
ciri <- read_csv("data/CIRI/CIRI_1981_2011.csv")
```

```{r}
### Polity 

# First 6 rows 
head(polity) #no piping 
polity %>% head() #with piping 

# Random 6 rows 
polity %>% sample_n(6)

# Look at data with glimpse
glimpse(polity)
polity %>% glimpse() 

### Ciri

# First 6 rows 
head(ciri) #no piping 
ciri %>% head() #with piping 

# Random 6 rows 
ciri %>% sample_n(6)

# Look at data with glimpse
glimpse(ciri)
ciri %>% glimpse() 
```



## 1.2. Cleaning Data

### 1.2.1. Look at data 

Let's start with the Polity dataset on political regime characteristics and transitions. First, let's inspect the dataset.

```{r}
# Get the object class
class(polity)

# Get the object dimensionality 
dim(polity) # Note this is rows by columns
```

```{r}

# Get the column names
colnames(polity)

# Get the row names
rownames(polity)[1:50] # Only the first 50 rows

# View first six rows and all columns
head(polity)

# View last six rows and all columns
tail(polity)

# Get detailed column-by-column information
str(polity) # base r 

glimpse(polity) # dplyr
```

### 1.2.2. Subset

We'll first want to subset, and maybe alter some values.

```{r}
# find column names
names(polity)

# quickly summarize the year column
summary(polity$year)

# subset the data (non-dplyr)
country.year <- subset(polity, year > 1979 & year < 2013)
country.year <- country.year[c("ccode", "country", "year", "polity", "democ", "autoc")]

# subset the data (dplyr)
country.year <- polity %>%
  filter(year > 1979 & year < 2013) %>%   
  select(ccode, country, year, polity, democ, autoc)

# take a look
head(country.year)
```

```{r}
# quickly summarize the polity column
summary(country.year$polity)

# apply NA values (non-dplyr)
country.year$polity[country.year$polity < -10] <- NA

# apply NA values (dplyr)
country.year %>%
  mutate(polity = replace(polity, polity < -10, NA))

# Note how the summary has changed - minimum value and NAs have changed
summary(country.year$polity)
```

```{r}
# get a list of all the countries in the dataset
head(unique(country.year$country)) #baser
country.year %>% distinct(country) %>% head #tidyverse

# delete records
country.year <- country.year[-which(country.year$country == "Sudan-North"), ]

# different ways of deleting the same records
country.year <- country.year[!(country.year$country == "Sudan-North"),]

country.year %<>% filter(country != "Sudan-North")
```


## 1.3. Merging data

Oftentimes, we want to combine data from multiple datasets to construct our own dataset. This is called **merging**. In order to merge datasets, at least one column has be to shared between them. This column is usually a vector of keys, or unique identifiers, by which you can match observations.

For our data, each observation is a "country-year". But the "country" column is problematic. Some datasets might use "United States", others "USA", or "United States of America" -- this makes it difficult to merge datasets.

So we'll use the "ccode" column, which is a numeric code commonly used to identify countries, along with "year". Together, this makes a unique id for each observation.

The first thing we want to do is inspect the dataset we want to merge and make it mergeable.

```{r}
# get column names
names(ciri) # to be merged

# subset for the observations we care about (aka, we probably don't need all the variables)
ciri.subset <- ciri %>%
  filter(YEAR > 1979 & YEAR < 2013) %>%
  select(YEAR, COW, UNREG, PHYSINT, SPEECH, NEW_EMPINX, WECON, WOPOL, WOSOC, ELECSD)

# rename columns so that they are comparable to country.year
names(country.year)

names(ciri.subset) <- c("year","ccode","unreg","physint","speech","new_empinx","wecon","wopol","wosoc","elecsd") # Mini-challenge - what code could you use to change all the column names to lowercase in one fell swooop? 

names(ciri.subset) 
```

Then merge the datasets.

```{r}
# merge format: merge(dataset1, dataset2, by=c(id variables), additional specifications as nec.)
country.year <- merge(country.year,ciri.subset, by = c("year","ccode"), all.x=TRUE)
```

Afterwards, we may want to remove duplicate rows. 

Base r deduplication: 
```{r, eval=F}
# delete duplicates using indexing
which(duplicated(country.year))

duplicates <- which(duplicated(country.year))

duplicates

country.year[-duplicates,]

# or you can use logical conditions 
identical(country.year[-duplicates,], 
  country.year[!duplicated(country.year),])

# or you can select unique values 
identical(country.year[-duplicates,],
  unique(country.year))
```

Tidyverse: 
```{r}
country.year %<>% distinct()
```

We can keep doing this for many datasets until we have a brand-spanking new dataset! 

## 1.4. Load cleaned dataset

Fast forward:

```{r}
country.year <- read.csv("data/country-year.csv")

names(country.year)

head(country.year)
```


**********************************************************

# 2. SUMMARISE {#summarizing}

First let's get a quick summary of all variables.

```{r}
summary(country.year)

skim(country.year) # skim function is very informative 
```

Look at region:

```{r}
summary(country.year$region)
```

Let's change this back to a factor.

```{r}
country.year$region <- as.factor(country.year$region)

summary(country.year$region)
```

Sometimes we need to do some basic checking for the number of observations or types of observations in our dataset. To do this quickly and easily, `table()` is our friend. 

Let's look the number of observations by region.

```{r}
table(country.year$region) 
```

We can even divide by the total number of rows to get proportion, percent, etc.

```{r}
table(country.year$region)/nrow(country.year) # Shown as decimal

table(country.year$region)/nrow(country.year)*100 # Shown as regular percentage
```

We can put two variables in there (check out what happens in early 1990s Eastern Europe!)

```{r}
table(country.year$year,country.year$region)
```

Finally, let's quickly take a look at a histogram of the variable `nyt.count`:

```{r}
hist(country.year$nyt.count, breaks = 100)
```

## 2.1. Calculating across groups {#calculating-across-groups}

Let's say we want to look at the number of NYT articles per region.

```{r}
summary(country.year$nyt.count)

sum(country.year$nyt.count[country.year$region == "MENA"], na.rm = T)

sum(country.year$nyt.count[country.year$region == "LA"], na.rm = T)
```

That can get tedious! A better way uses the popular new `dplyr` package, which uses a the ***split-apply-combine*** strategy. We **split** the data using some variable or variables to group our data, we **apply** some kind of function (either a built-in one, or one we write ourselves), and then we re-**combine** the data into a new dataset

All of the major dplyr functions have the same basic syntax. In this case, we're going to use summarise, which will let us do what took a few steps before in a single step!

Let's say we wanted to sum up all the NYT articles per region, and return those counts into their own dataframe. (This is often how we want our data organized for plotting, too.)

```{r}
summarise(group_by(country.year, region), region.sum = 
            sum(nyt.count, na.rm=T)) # bad

```
* The error message is not serious, but has something to do with tibble. For more information, read [the following](#https://www.r-bloggers.com/the-trouble-with-tibbles/).

Note that many functions, like `sum`, are sensitive to missing values (NA); you should be sure to specify na.rm=T to avoid errors. 

We can use even easier-to-read syntax using the chain or pipe operator (`%>%`), which passes the object or function from the line above into the next line for you: 

```{r}
count.region <- country.year %>% # put the dataset you want to work within here
  group_by(region)  %>% # next we have our grouping function
  summarise(region.sum = sum(nyt.count, na.rm=T)) # now we name our new variable and specify the function to run
count.region

```

We can also split by multiple variables:

```{r}
count.region.year <- country.year %>% 
  group_by(region, year)  %>% 
  summarise(nyt.count = sum(nyt.count, na.rm=T)) 

country.year
```

Another very useful function is **arrange**, which orders a data frame on the basis of column contents.

```{r}
count.region.year
```

```{r}
# arrange by count, desc
by.count <- arrange(count.region.year, desc(nyt.count))

head(by.count)
```

```{r}
# arrange by year, then count
by.year.count <- arrange(count.region.year, year, desc(nyt.count))

head(by.year.count)
```

dplyr has a number of other useful functions (all of which follow the same syntax), which you'll see more of in your homework for this week. 

**********************************************************

# 3) RESHAPING {#reshaping} 

Our country.year dataset, is currently in what's called "long" form: nyt article values are specified in the nyt.count column, and the country and year (aka, what uniquely identifies each value) are specified in each row. Let's say we wanted to make a new "wide" database, where each country has its own row, and the article counts within each year exist in multiple columns in that row. 

Starting:
country | year | nyt.count
Brazil | 1976 | 434
Brazil | 1977 | 628
France | 1976 | 952
France | 1977 | 893

Ending:
country | 1976.count | 1977.count
Brazil | 424 | 628
France | 952 | 893


Base R does have commands for reshaping data (including **aggregate**, **by**, **tapply**, etc.), but each of their input commands are slightly different and are only suited for specific reshaping tasks. The **reshape2** package overcomes these argument and task inconsistencies, but is fairly slow. A recent alternative is **tidyr**, which has an easy syntax, interfaces well with dplyr, and works much faster:

The package contains two major commands, **gather** (for our current purposes, that means reformat from wide to long) and **spread** (reformat from long to wide). Here, want the **spread** funciton.

```{r eval=F}
# here's our data, from when we used dplyr to organize it the way we wanted:
count.region.year <- country.year %>% 
  group_by(region, year)  %>% 
  summarise(nyt.count=sum(nyt.count, na.rm=T)) 

# now spread it:
region.count.wide <- spread(count.region.year, year, nyt.count)

region.count.wide[,1:10]

# write to csv
write.csv(region.count.wide, "region_year_counts.csv")

count.region.year
```

**********************************************************

# 5. STATS 

Description and Inference. 

Once we've imported our data, summarized it, carried out group-wise operations, and perhaps reshaped it, we may also want to assess quantitatively the relationships between our variables. We tend to describe these tests as falling into one of two categories: **descriptive**, which implies that we don't have a way to understand causation (e.g., did x cause y, or y cause x?), and **inferential**, in which we believe that we do have a way to assess whether the relationship between x and y (for instance) is **causal** (e.g., by using a randomized controlled trial). 

## 5.1. Descriptive tests {#description}

This often requires doing the following:
1) Assessing correlations
2) Carrying out classical hypothesis tests
3) Estimating regressions

Note that this class is not intended to serve as quantitative training and thus does not go into any nitty-gritty details of (for example) assessing causal identification; these are meant to be bare-bones instructions on how to run basic functions. 

## 5.2. Correlations

Often, when we want to quickly understand the relationship between two variables, and for whatever reason we want to summarize that quantitatively, we run a correlation (though, as you'll learn next week, exploring your data should be done **visually** FIRST, and correlations are no substitute for visualization). 

```{r}

# What's the relationship between population and GDP? 
cor(country.year$gdp.pc.un, country.year$pop.wdi, use="pairwise.complete.obs", method="pearson")

```


What happens if we change which observations are included? Does the correlation change if we use a different kind of test? Use `?cor` to see your options. 

## 5.3. Hypothesis Testing

Let's say we're interested in whether the New York Times covers MENA differently than the West in terms of quantity. One can test for differences in distributions in either a) their means using t-tests, or b) their entire distributions using ks-tests

```{r}
nyt.africa <- country.year$nyt.count[country.year$region=="Africa"]

nyt.mena <- country.year$nyt.count[country.year$region=="MENA"]

# this is a simple little plot, just to get us started: 
plot(density(nyt.africa, na.rm = T), col="blue", lwd=1, main="NYT Coverage of Africa and MENA")

lines(density(nyt.mena, na.rm = T), col="red", lwd=1)

# these are highly skewed, so let's transform taking the logarithm  
nyt.africa.logged <- log(country.year$nyt.count[country.year$region=="Africa"])
nyt.mena.logged <- log(country.year$nyt.count[country.year$region=="MENA"])

plot(density(nyt.africa.logged, na.rm = T), col="blue", lwd=1, main="NYT Coverage of Africa and MENA")
lines(density(nyt.mena.logged, na.rm = T), col="red", lwd=1)

# t test of means
t.test(x=nyt.africa.logged, y=nyt.mena.logged)

# ks tests of distributions
ks.test(x=nyt.africa.logged, y=nyt.mena.logged)
```

## 5.4. Regressions {#inferential}

Running regressions in R is extremely simple, very straightforward (though doing things with standard errors requires a little extra work). **lm** is the most basic OLS regression you can run, and the most basic catch-all non-linear regression function in R is *glm*, which fits a generalized linear model with your choice of family/link function (gaussian, logit, poisson, etc.). 

Remember, just like **you** are the only one who can prevent forest fires, **you** are the one responsible for thinking ahead of time about whether your regression is descriptive or inferential. This is determined by your research design, not your code. 

Once you understand what you're estimating, the basic lm and glm calls look something like this:

```{r eval=FALSE}
lm(data=yourdata, y~x1+x2+x3+...)

glm(data=yourdata, y~x1+x2+x3+..., family=familyname)
```

In glm, here are a bunch of families and links to use (see `?family` for a full list), but some essentials are **binomial(link = "logit")**, **gaussian(link = "identity")**, and **poisson(link = "log")**

Example: suppose we want to explain the variation in NYT articles, and we think our variables give us sufficient leverage to make that assessment.  A typical lm call would look something like this:

```{r}
names(country.year)

reg <- lm(data = country.year, nyt.count ~ gdp.pc.un + pop.wdi + domestic9 + idealpoint)

summary(reg) # You'll almost always want to display your regression results using summary, since lm and glm calls return lengthy lists containing all the same information but in a much less readable format
```