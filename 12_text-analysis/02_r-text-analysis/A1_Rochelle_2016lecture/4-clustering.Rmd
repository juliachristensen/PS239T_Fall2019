---
title: "Clustering"
author: "PS239T"
date: "Fall 2016"
output: html_document
---

### Setup Environment

First let's load our required packages.

```{r}
setwd("~/Desktop/PS239T/12_text-analysis") # Your working directory here
rm(list=ls())
library(tm)
library(RTextTools)
library(lsa)
library(cluster)
library(fpc)
```

## 0. Prepare Data

```{r}
# load corpus
docs <- Corpus(DirSource("Data/British_Fiction"))
docs
# make DTM
dtm <- DocumentTermMatrix(docs)
dim(dtm)
inspect(dtm[,100:104])
# convert to matrix
dtm.m <- as.matrix(dtm)
```

## 1. Similarity and Distance

Arranging our texts in a document-term matrix make available a range of exploratory procedures. For example, calculating a measure of similarity between texts becomes simple. Since each row of the document-term matrix is a sequence of a novel’s word frequencies, it is possible to put mathematical notions of similarity (or distance) between sequences of numbers in service of calculating the similarity (or distance) between any two novels. 

### 1.1 Euclidean Distance

One frequently used measure of distance between vectors (a measure easily converted into a measure of similarity) is Euclidean distance. The [Euclidean distance](https://en.wikipedia.org/wiki/Euclidean_distance) between two vectors in the plane should be familiar from geometry, as it is the length of the hypotenuse that joins the two vectors. For instance, consider the Euclidean distance between the vectos $\vec{x} = (1,3)$ and $\vec{y} = (4,2)$. The distance between the two vectors is $\sqrt{(1-4)^2 + (3-2)^2} = \sqrt{10}$

![alt text](img/euclid.png)

> **Note**
> Measures of distance can be converted into measures of similarity. If your measures of distance are all between zero and one, then a measure of similarity could be one minus the distance. (The inverse of the distance would also serve as a measure of similarity.)

This concept of distance is not restricted to two dimensions. For example, it is not difficult to imagine the figure above translated into three dimensions. We can also persuade ourselves that the measure of distance extends to an arbitrary number of dimensions.

Since two novels in our corpus now have an expression as vectors, we can calculate the Euclidean distance between them. We can do this by hand or we take advantages of the `dist` function in `r`.

```{r}
d <- dist(dtm.m, method = "euclidean")
d
```

### 1.2 Cosine Similarity

And if we want to use a measure of distance that takes into consideration the length of the novels (an excellent idea), we can calculate the [cosine similarity](http://www.gettingcirrius.com/2010/12/calculating-similarity-part-1-cosine.html) by using the `cosine` function from the `lsa` package.

Unlike the `dist` function which compares distances between rows, the `cosine` function compares distances between columns. This means that we have to **transpose** our matrix before passing it into the `cosine` function.

```{r}
# transpose matrix
dtm.t <- t(dtm.m)
# calculate cosine metric
d <- cosine(dtm.t)
d
```

Keep in mind that cosine similarity is a measure of similarity (rather than distance) that ranges between 0 and 1 (as it is the cosine of the angle between the two vectors). In order to get a measure of distance (or dissimilarity), we need to “flip” the measure so that a larger angle receives a larger value. The distance measure derived from cosine similarity is therefore one minus the cosine similarity between two vectors.

```{r}
# convert to dissimilarity distances
d <- as.dist(1-d) 
d
```

## 2. Visualizing distance with MDS

It is often desirable to visualize the pairwise distances between our texts. A general approach to visualizing distances is to assign a point in a plane to each text, making sure that the distance between points is proportional to the pairwise distances we calculated. This kind of visualization is common enough that it has a name, ["multidimensional scaling"](https://en.wikipedia.org/wiki/Multidimensional_scaling) (MDS)

```{r}
fit <- cmdscale(d,eig=TRUE, k=2) # k is the number of dim
fit # view results

# plot solution 
x <- fit$points[,1]
y <- fit$points[,2]
plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2", 
  main="Metric	MDS",	type="n")
text(x, y, labels = row.names(dtm), cex=.7)
```

## 3. Clustering texts based on distance

Clustering texts into discrete groups of similar texts is often a useful exploratory step. For example, a researcher may be wondering if certain textual features partition a collection of texts by author or by genre. Pairwise distances alone do not produce any kind of classification. To put a set of distance measurements to work in classification requires additional assumptions, such as a definition of a group or cluster.

The ideas underlying the transition from distances to clusters are, for the most part, common sense. Any clustering of texts should result in texts that are closer to each other (in the distance matrix) residing in the same cluster. There are many ways of satisfying this requirement; there no unique clustering based on distances that is the "best." 

### 3.1 Hierarchical clustering based on distance

One strategy for clustering is called [Ward’s method](https://en.wikipedia.org/wiki/Ward%27s_method). Rather than producing a single clustering, Ward’s method produces a hierarchy of clusterings. All that Ward’s method requires is a set of pairwise distance measurements–such as those we calculated a moment ago. Ward’s method produces a hierarchical clustering of texts via the following procedure:

1. Start with each text in its own cluster
2. Find the closest clusters and merge them. The distance between two clusters is the change in the sum of squared distances when they are merged. Continue until only a single cluster remains.
3. Return a tree containing a record of cluster-merges.

The `hclust` function can perform this algorithm for us. Let's use it on the `cosine` metric we discussed above.

```{r}
# transpose matrix
dtm.t <- t(dtm.m)
# calculate cosine metric
d <- cosine(dtm.t)
# convert to dissimilarity distances
d <- as.dist(1-d) 
# plot hierarchical cluster
plot(hclust(d))
```

### 3.2 K-means clusters

Another popular way to cluster text is a method called [k-means](https://en.wikipedia.org/wiki/K-means_clustering) which  aims to partition `n` observations into `k` clusters in which each observation belongs to the cluster with the nearest mean, serving as a prototype of the cluster.

We can use the `kmean` function in r to perform this operation.

In the example below, we cluster based on the `cosine` distance matrix made above.

```{r}
set.seed(0001)
clust <- kmeans(d,2)

# check out clusters
clusters <- clust$cluster
clusters
plotcluster(d, clust$cluster)
```

k-means can also be used on the vectorized texts themselves. Notice the difference in results when we input the raw dtm.

```{r}
set.seed(0001)
clust <- kmeans(dtm.m,2)

# check out clusters
clusters <- clust$cluster
clusters
plotcluster(d, clust$cluster)
```

It should be noted that k-means are very sensitive to feature scaling. Notice the difference in results once we've applied a tf-idf weighting procedure, which weights a term-document matrix by the inverse of the document term frequency. 

```{r}
# tf-idf weights
dtm.weighted <- DocumentTermMatrix(docs,
                      control = list(weighting =function(x) weightTfIdf(x, normalize = TRUE)))
dtm.weighted <- as.matrix(dtm.weighted)
dtm.weighted[,1:5]

# cluster
set.seed(0001)
clust <- kmeans(dtm.weighted,2)

# check out clusters
clusters <- clust$cluster
clusters

plotcluster(d, clust$cluster)
```

### 3.3 Exercise: Perform kmeans on a simple normalized matrix.

Yet another way to scale features is simple frequency normalization. Try it out yourself.

```{r}
# normalized without weights
dtm.normalized <- dtm.m/rowSums(dtm.m)
dtm.normalized[,1:5]  

### YOUR CODE HERE TO CLUSTER

```

Which feature scaling worked best?

### 3.4 Interpreting Clusters

Let's do another clustering exercise using a sample of New York Times articles, available through `RTextTools`.

```{r}
library(RTextTools)
data("NYTimes")

docs <- Corpus(VectorSource(NYTimes$Title))

# let's use a fully-processed DTM.
dtm <- DocumentTermMatrix(docs,
           control = list(tolower = TRUE,
                          stopwords = TRUE,
                          removeNumbers = TRUE,
                          removePunctuation = TRUE,
                          stemming=TRUE))

# and now normatlize it
dtm.m <- as.matrix(dtm)

# cluster
set.seed(2)
clust <- kmeans(dtm.m, 10)

# save clusters for later
clusters <- clust$cluster
```

We can intrepret and apply hand-labels to each cluster using computer and hand methods.

(The following forumulation is from Justin Grimmer's [Text as Data](http://stanford.edu/~jgrimmer/Text14/) class)

The final cluster centers are computed as the mean for each feature within each final cluster. That is, they reflect the characteristics of the "exemplar" document for each cluster.

Suppose $\boldsymbol{\theta}_{k}$ is the cluster center for cluster $k$ and define $\bar{\boldsymbol{\theta}}_{-k} = \frac{\sum_{j \neq k} \boldsymbol{\theta_{j}}   }{K-1 }$ or the average of the centers not $k$.  

Define $\text{Diff}_{k}  =  \boldsymbol{\theta}_{k} - \bar{\boldsymbol{\theta}}_{-k}\nonumber$

We can then use the top ten words from $\text{Diff}_{k}$ to label the clusters. 


```{r}
# make dataframe of cluster centers
centers <- as.data.frame(clust$centers)
centers[,1:5]

# first set the number of clusters
n = max(clusters)

# write a function that inputs cluser number k (e.g. k = 2) outputs the top 10 words
top10words <- function(k){
  theta.k <- centers[k,] # define theta-k, i.e.  row k of cluster centers dataframe
  theta.notk <- colSums(centers[-(k),])/(n-1) # define theta-not-k, i.e. rows not-k of cluster centers divided by number of clusters - 1)
  diffk <- as.data.frame(theta.k - theta.notk) # define difference diffk
  return(colnames(diffk[,order(diffk,decreasing=TRUE)][1:10])) # order decreasing, take top 10
}

# set up a matrix to contain data
keywords<- matrix(NA, nrow=10, ncol=n)

# fill it up
for (i in 1:10){
  keywords[,i] <- top10words(i)
}

# check em out
keywords
```

However, it is important to note that there is no way to find out what the clusters *really* mean, because at the end of the day, humans are the final judge. This means **reading** the texts in each cluster and using your brain to determine semantic meaning.

To this end, we can sample and read texts assigned to each cluster to produce a hand label.

```{r}
# set the cluster you want to sample
k = 3

# and then sample 2 titles
as.character(NYTimes$Title[sample(which(clust$cluster==k),2)])
```
